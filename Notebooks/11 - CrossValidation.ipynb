{
  "cells": [
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "# Cross-Validation of Machine Learning Models\n\n## Overfitting\n\nAs we develop models to try and predict outcomes, we are faced with an important trade-off that might not be obvious at first. How good should we make our model?\n\nBelieve it or not, there are real risks to creating models that appear to be highly performant as we design them. The risk with these amazing models is that they will perform very well on data that we have access to right now, but will perform at a much lower level when used in the real world. This happens when our model is **overfit** to the available data. **Overfitting** occurs when our model assumes that the available data says more about the real world than the data is actually capable of predicting.\n\nFor example, for many consecutive years it was possible to predict stock market performance in the United States based on outcomes in the Super Bowl (the championship game of the National Football League). Anyone with any sense would agree that it is highly unlikely that a single sporting event (even if it is one of the largest annual sporting events in the world) could change the performance of the US stock markets. These stock markets have orders of magnitude greater value than ANY sporting event. If our model of the stock market decided that the outcome of this game was a predictor of the stock market, then we would say that our model is **overfitted** to the data, and is making associations that are unlikely to remain true in the future.\n\nWe can overfit a model in several ways.\n\nFirst, if we are careless about the variables included in our model, then we may end up with spurious correlations like that between the stock market and the Super Bowl. In these cases, we begin to believe that a relationship between two unrelated outcomes exists. This relationship may *appear* true in some arbitrary time span, but is unlikely to continue into the future, since the relationship was likely random. Thus, our model believes something about the world will predict outcomes, but we know that this is not probable.\n\n![](https://www.cookandbynum.com/wp-content/uploads/2018/11/per-capital-cheese.png)\n\nSecond, we can overfit a model by choosing a model of high complexity. As we increase the complexity of our models, we increase the probability that our model will \"bend\" itself in order to match our data. We can imagine a random collection of observations that have a linear relationship in realtiy, but the relationship is noisy. In this case, a linear model is the best model to represent the system. On the other hand, we might be able to reduce the error in our model when measured against available data by choosing a non-linear model. Doing so will improve our perceived accuracy when we test our model, but will decrease its ability to make accurate predictions in the real world. The image below represents this kind of overfitting:\n\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/6/68/Overfitted_Data.png/300px-Overfitted_Data.png)"
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "## How does Cross-Validation help?\n\nCross-validation is a key component of preventing overfitting. The key concept behind cross-validation is that we create multiple models using sub-samples of our data in order to verify that the model will behave similarly when given new data that it has not yet observed. The concept is similar to our train-test splitting that we have done with previous models, but performed several times.\n\nThe most common form of cross-validation is called **$k$-fold cross validation**. The concept is very simple:\n\n1. Separate training and testing data\n2. Break the training data into $k$ equally-sized portions (typically done through random sampling).\n3. Train the model $k$ times using identical model parameters, and where each iteration uses a different combination of training and testing data\n4. Record the performance of each iteration\n5. Calculate the average performance of the $k$ models\n6. If performance is satisfactory, train the model on the full trainind data set, and then test performance on the testing data for final validation of predictive ability\n    - If the model is not satisfactory, refine the model parameters and go back to step 2.\n\n<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" width=500></img>\n\nBy implementing $k$-fold cross validation, we are better able to understand whether or not our model is robust to new observations. For example, I might see that the accuracy of one split is very different from the accuracy of another split. This will give me information about how reliable the model will be as new real-world data is fed into the model. This provides more realistic expectations for the performance of a model than we might otherwise have.\n\n## Using Representative Data\n\nNo matter how we attempt to predict performance in the real world, the most important factor is that we ensure that the data we use to train our model actually resembles the data that we will observe in practice. No matter how well-trained our model is, the model **will fail** if it has not been trained on representative data!"
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "## Implementing Cross-Validation\n\nWe will implement **stratified** $k$-fold cross validation. **Stratification** simply means that we will break our data apart so that our dependent variable (or label) is evenly mixed in each fold of data, and is particularly valuable where some outcomes are infrequent, and we want to ensure that our model is always given the opportunity to observe those outcomes. In more balanced data, using stratification is of less importance.\n\nWhile in practice we should first split our data into training and testing data, we will skip this step below in order to keep our example as simple as possible."
    },
    {
      "metadata": {
        "trusted": false,
        "state": "normal"
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier as dt\nfrom sklearn.metrics import accuracy_score\n\nmnist = pd.read_csv(\"https://github.com/dustywhite7/pythonMikkeli/blob/master/exampleData/mnistTrain.csv?raw=true\")\n\n# Separate our features from our labels\ny = mnist['Label']\nx = mnist.drop('Label', axis=1)\n\n# Make 5 folds in the data\nskf = StratifiedKFold(n_splits=5)\n\n# Create the model\nclf = dt(max_depth=15)\n\n# Create a list to store accuracy values\naccuracy = []\nn=1\n\n# For loop to train the model on each fold\nfor train_index, test_index in skf.split(x, y):\n    # Store the folded data\n    x_train = x.loc[train_index, :]\n    x_test = x.loc[test_index, :]\n    y_train =  y[train_index]\n    y_test = y[test_index]\n    \n    # Fit the model\n    clf.fit(x_train, y_train)\n    \n    # Calculate model accuracy on left-out data\n    acc = accuracy_score(clf.predict(x_test), y_test)\n    \n    # Print results\n    print(\"Fold {0} Accuracy: {1}%\".format(n, round(acc*100, 2)))\n    \n    # Store results\n    accuracy.append(acc)\n    \n    # Add one to our label count\n    n+=1\n    \n# Print overall results\nprint(\"\\nAverage Accuracy: {}%\".format(round(np.mean(accuracy)*100, 2)))\nprint(\"Accuracy Standard Deviation: {}%\".format(round(np.std(accuracy)*100), 2))\n",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Fold 1 Accuracy: 74.08%\nFold 2 Accuracy: 74.55%\nFold 3 Accuracy: 75.3%\nFold 4 Accuracy: 76.35%\nFold 5 Accuracy: 72.62%\n\nAverage Accuracy: 74.58%\nAccuracy Standard Deviation: 1.0%\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "The `StratifiedKFold` object is created with the number of folds that we specify through the argument `n_splits`. This object contains lists that indicate the index values of the observations that belong to each fold. We are then able to *iterate* over the object to create our training and testing groups. \n\nThe rest of our code simply trains models on each split of the data, and reports the accuracy of the model in each case. We then report the average accuracy across all splits together with the standard deviation of accuracy. These measures tell us that our accuracy is highly consistent across splits, and so we should expect the model to continue to perform at near these levels on similar data."
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "**Solve it:**\n\nImplement cross-validation using either decision tree or random forest classifiers to gauge the robustness of models trained to predict whether or not individuals called by insurance salespeople will sign up to buy car insurance during the call. The data can be accessed [here](https://raw.githubusercontent.com/dustywhite7/pythonMikkeli/master/exampleData/carInsuranceTrain.csv). \n\nYou should implement 10-fold cross-validation, and should report the accuracy of each fold, as well as the average model accuracy. Place your code in the cell below. You will be graded based on the following:\n\n- Preparing the data to be used in a classification model [1 point]\n- A working `sklearn` decision tree or random forest classifier [1 point]\n- 10-fold cross-validation implemented on the classifier [1 point]\n- Accuracy reported for each fold [1 point]\n- Overall average accuracy reported [1 point]"
    },
    {
      "metadata": {
        "trusted": false,
        "state": "graded",
        "deletable": false,
        "id": "tiny_ran"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "mimir": {
      "project_id": "5f2cdba3-2846-4b77-88ba-7caec59e2514",
      "last_submission_id": "",
      "data": {}
    },
    "varInspector": {
      "window_display": false,
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "library": "var_list.py",
          "delete_cmd_prefix": "del ",
          "delete_cmd_postfix": "",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "library": "var_list.r",
          "delete_cmd_prefix": "rm(",
          "delete_cmd_postfix": ") ",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}